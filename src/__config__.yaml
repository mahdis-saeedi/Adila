hydra:
  output_subdir: null
  run:
    dir: . #unix or macos
    #dir: /dev/null # windows
seed: 0
acceleration: 'cpu' #cpu for all cores minus one, cpu:3 for 3 cores, 'cuda' or 'cuda:0' for the only available gpu, 'cuda:3' for gpu3 >> TODO: multiple gpus

data:
  fteamsvecs: ../output/dblp/toy.dblp.v12.json/teamsvecs.pkl
  fgender: ../output/dblp/toy.dblp.v12.json/females.csv
  fsplits: ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85.pkl
  fpreds: ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/rnd.b1000 # can be a single file too like f0.test.pred
  output: ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/rnd.b1000

fair: #post-hoc reranks of the prediction list
  algorithm: det_greedy #reranking algorithm from {det_greedy, det_cons, det_relaxed, fa-ir}; required; Eg. det_cons
  notion: eo # equality of odds (eo), demographic parity (dp)
  k_max: 100 #cutoff for the reranking algorithms; default: None
  attribute: popularity #the sensitive attribute from {popularity, gender}
  dp_ratio: #hard ratio for desired (e.g., female or non-popular) experts after reranking for dp; if None, based on distribution in dataset; default: None; Eg. 0.5
  is_popular_alg: avg #popularity status based on `avg` teams per experts, i.e., whoever above this value is popular, or whoever is in the head of `auc` distribution figure
  is_popular_coef: 1.0
  alpha: 0.1, #the significance value for fa-ir algorithm

eval:
  fair_metrics': [ndkl, skew, exp, expu]
  topk: '2,5,10,20,50,100'
  utility_metrics:
    trec: [P_topk, recall_topk, ndcg_cut_topk, map_cut_topk, success_topk] # see trec_eval for a complete list or trec_* metrics https://github.com/terrierteam/pytrec_eval/blob/master/tests/pytrec_eval_tests.py
    other: [skill_coverage_topk]
  per_instance: True # needed for paired significance tests