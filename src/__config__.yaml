hydra:
  output_subdir: null
  run:
    dir: . #unix or macos
    #dir: /dev/null # windows
seed: 0
acceleration: 'cpu' #cpu for all cores minus one, cpu:3 for 3 cores
data:
  fteamsvecs: ../output/dblp/toy.dblp.v12.json/teamsvecs.pkl
  fgender:    ../output/dblp/toy.dblp.v12.json/females.csv
  fsplits:    ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85.pkl
  fpred:      ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/rnd.b1000/ # a subfolder containing *.pred
  #fpred:      ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/rnd.b1000/f0.test.pred # or a single file like f0.test.pred
  output:     ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/rnd.b1000

fair: #post-hoc reranks of the prediction list
  algorithm: det_greedy #reranking algorithm from {det_greedy, det_cons, det_relaxed, det_const_sort, fa-ir}; required; Eg. det_cons
  notion: eo # equality of odds (eo), demographic parity (dp)
  k_max: 5 #cutoff for the reranking algorithms; default: None
  attribute: popularity #the sensitive attribute from {popularity, gender}
  dp_ratio: #hard ratio for desired (e.g., female or non-popular) experts after reranking for dp; if None, based on distribution in dataset; default: None; Eg. 0.5
  is_popular_alg: avg #popularity status based on `avg` teams per experts, i.e., whoever above this value is popular, or whoever is in the head of `auc` distribution figure
  is_popular_coef: 1.5
  alpha: 0.1 #the significance value for fa-ir algorithm

eval:
  per_instance: True # needed for paired significance tests
  topK: ${fair.k_max} # first stage retrieval to enhance efficiency
  fair_metrics: [ndkl, skew] #, exp, expu >> python 3.9+ from FairRankTune
  utility_metrics:
    topk: '1,2,${fair.k_max}'
    trec: [P_topk, recall_topk, ndcg_cut_topk, map_cut_topk, success_topk] # see trec_eval for a complete list or trec_* metrics https://github.com/terrierteam/pytrec_eval/blob/master/tests/pytrec_eval_tests.py
    other: []
